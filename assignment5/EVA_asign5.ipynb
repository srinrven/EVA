{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_asign5_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlEUplvoxKAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "d01a0f33-a83c-4320-ab72-a39b65be3adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[1])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb732f68be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoAg/EBThB4Ii/EBQhB8I6v8AG8x2aarNGp8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDZxPhhxOgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzMqbTnxQQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "#X_train /= 255\n",
        "#X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LdYiW6ixR9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train[:10]\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFR0F9j0xVp2",
        "colab_type": "code",
        "outputId": "bbbf1c0e-4ddd-422c-a0a3-e518399862f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "ed4e3859-386b-49c8-d572-424e3d9cf41b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1397
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "from keras import regularizers\n",
        "model = Sequential()\n",
        "weight_decay = 1e-4\n",
        " \n",
        "model.add(Convolution2D(16, 3, 3, input_shape=(28,28,1),kernel_regularizer=regularizers.l2(weight_decay))) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(32, 3, 3, kernel_regularizer=regularizers.l2(weight_decay))) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, kernel_regularizer=regularizers.l2(weight_decay))) #22\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#11\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3,kernel_regularizer=regularizers.l2(weight_decay)))#9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, kernel_regularizer=regularizers.l2(weight_decay)))#7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, kernel_regularizer=regularizers.l2(weight_decay)))#5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, kernel_regularizer=regularizers.l2(weight_decay)))#3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 4, 4, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), input_shape=(28, 28, 1..., kernel_regularizer=<keras.reg...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), kernel_regularizer=<keras.reg...)`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), kernel_regularizer=<keras.reg...)`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_25 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 24, 24, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 24, 24, 10)        330       \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 24, 24, 10)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 16,604\n",
            "Trainable params: 16,360\n",
            "Non-trainable params: 244\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4), kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AQVjVHIhCCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import model_from_json\n",
        "def savemodel():\n",
        "  model_json = model.to_json()\n",
        "  with open('model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "  model.save_weights('model.h5') \n",
        "\n",
        "\n",
        "def loadmodel():\n",
        "  json_file = open('model.json', 'r')\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  model = model_from_json(loaded_model_json)\n",
        "  model.load_weights('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoSxUtiDZayf",
        "colab_type": "code",
        "outputId": "673199c1-7c8a-4ac7-d636-4d493f8e39d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4114
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "datagen = ImageDataGenerator(rescale= 1./255,    \n",
        "                             featurewise_center=True,  \n",
        "                             featurewise_std_normalization=True                           \n",
        "                              )\n",
        "datagen.fit(X_train)\n",
        "datagen.fit(X_test)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "epochs = 40\n",
        "best_accuracy = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "for epoch_num in range(epochs):\n",
        "  print(\"Epoch no: \" + str(epoch_num))\n",
        "  result = model.fit(X_train, Y_train, batch_size=128, epochs=1, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "  accuracy = float(result.history['val_acc'][0])\n",
        "  if (accuracy > best_accuracy):\n",
        "    best_accuracy = accuracy\n",
        "    best_epoch = epoch_num\n",
        "    savemodel()\n",
        "    \n",
        "print(\"Epoch \" + str(best_epoch) + \" has highest accuracy so far and accuracy is \" + str(best_accuracy))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.3903 - acc: 0.9298 - val_loss: 0.1217 - val_acc: 0.9828\n",
            "Epoch no: 1\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.1229 - acc: 0.9789 - val_loss: 0.0741 - val_acc: 0.9874\n",
            "Epoch no: 2\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.0896 - acc: 0.9830 - val_loss: 0.0691 - val_acc: 0.9876\n",
            "Epoch no: 3\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0791 - acc: 0.9843 - val_loss: 0.0498 - val_acc: 0.9911\n",
            "Epoch no: 4\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0727 - acc: 0.9854 - val_loss: 0.0532 - val_acc: 0.9899\n",
            "Epoch no: 5\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.0676 - acc: 0.9868 - val_loss: 0.0733 - val_acc: 0.9839\n",
            "Epoch no: 6\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.0656 - acc: 0.9872 - val_loss: 0.0472 - val_acc: 0.9924\n",
            "Epoch no: 7\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0651 - acc: 0.9872 - val_loss: 0.0528 - val_acc: 0.9903\n",
            "Epoch no: 8\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0648 - acc: 0.9873 - val_loss: 0.0513 - val_acc: 0.9907\n",
            "Epoch no: 9\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0620 - acc: 0.9884 - val_loss: 0.0470 - val_acc: 0.9924\n",
            "Epoch no: 10\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0641 - acc: 0.9871 - val_loss: 0.0475 - val_acc: 0.9925\n",
            "Epoch no: 11\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0624 - acc: 0.9883 - val_loss: 0.0642 - val_acc: 0.9874\n",
            "Epoch no: 12\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0637 - acc: 0.9876 - val_loss: 0.0471 - val_acc: 0.9923\n",
            "Epoch no: 13\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0619 - acc: 0.9883 - val_loss: 0.0580 - val_acc: 0.9894\n",
            "Epoch no: 14\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0614 - acc: 0.9887 - val_loss: 0.0600 - val_acc: 0.9891\n",
            "Epoch no: 15\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0618 - acc: 0.9885 - val_loss: 0.0631 - val_acc: 0.9893\n",
            "Epoch no: 16\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0611 - acc: 0.9888 - val_loss: 0.0607 - val_acc: 0.9894\n",
            "Epoch no: 17\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0616 - acc: 0.9885 - val_loss: 0.0510 - val_acc: 0.9917\n",
            "Epoch no: 18\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0599 - acc: 0.9890 - val_loss: 0.0537 - val_acc: 0.9896\n",
            "Epoch no: 19\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0600 - acc: 0.9894 - val_loss: 0.0498 - val_acc: 0.9924\n",
            "Epoch no: 20\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0594 - acc: 0.9896 - val_loss: 0.0543 - val_acc: 0.9903\n",
            "Epoch no: 21\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0610 - acc: 0.9889 - val_loss: 0.0510 - val_acc: 0.9919\n",
            "Epoch no: 22\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0584 - acc: 0.9897 - val_loss: 0.0573 - val_acc: 0.9895\n",
            "Epoch no: 23\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0586 - acc: 0.9897 - val_loss: 0.0534 - val_acc: 0.9910\n",
            "Epoch no: 24\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0583 - acc: 0.9897 - val_loss: 0.0571 - val_acc: 0.9904\n",
            "Epoch no: 25\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.0591 - acc: 0.9896 - val_loss: 0.0494 - val_acc: 0.9918\n",
            "Epoch no: 26\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0591 - acc: 0.9896 - val_loss: 0.0462 - val_acc: 0.9934\n",
            "Epoch no: 27\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0593 - acc: 0.9892 - val_loss: 0.0503 - val_acc: 0.9924\n",
            "Epoch no: 28\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0598 - acc: 0.9892 - val_loss: 0.0483 - val_acc: 0.9936\n",
            "Epoch no: 29\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0590 - acc: 0.9897 - val_loss: 0.0509 - val_acc: 0.9935\n",
            "Epoch no: 30\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0592 - acc: 0.9894 - val_loss: 0.0514 - val_acc: 0.9923\n",
            "Epoch no: 31\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0590 - acc: 0.9897 - val_loss: 0.0485 - val_acc: 0.9927\n",
            "Epoch no: 32\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.0576 - acc: 0.9897 - val_loss: 0.0495 - val_acc: 0.9922\n",
            "Epoch no: 33\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0593 - acc: 0.9893 - val_loss: 0.0474 - val_acc: 0.9922\n",
            "Epoch no: 34\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0568 - acc: 0.9897 - val_loss: 0.0466 - val_acc: 0.9936\n",
            "Epoch no: 35\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0585 - acc: 0.9894 - val_loss: 0.0537 - val_acc: 0.9894\n",
            "Epoch no: 36\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.0562 - acc: 0.9902 - val_loss: 0.0489 - val_acc: 0.9927\n",
            "Epoch no: 37\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0570 - acc: 0.9900 - val_loss: 0.0498 - val_acc: 0.9919\n",
            "Epoch no: 38\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0562 - acc: 0.9898 - val_loss: 0.0778 - val_acc: 0.9837\n",
            "Epoch no: 39\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0566 - acc: 0.9898 - val_loss: 0.0470 - val_acc: 0.9925\n",
            "Epoch 28 has highest accuracy so far and accuracy is 0.9936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jjrrlkwwUkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_misclassified(X, Y_ohe, Y_pred, classes,\n",
        "                       columns=5, total=25,\n",
        "                       pick_randomly=True, image_size_multiplier=4):\n",
        "    y_true = np.argmax(Y_ohe, axis=1)\n",
        "    yp = np.argmax(Y_pred, axis=1)\n",
        "    misclassified = y_true != yp\n",
        "    X = X[misclassified]\n",
        "    Y_ohe = Y_ohe[misclassified]\n",
        "    Y_pred = Y_pred[misclassified]\n",
        "    y_true = y_true[misclassified]\n",
        "    yp = yp[misclassified]\n",
        "    total = min(total, len(X))\n",
        "    rows = int(np.ceil(total / columns))\n",
        "\n",
        "    indexes = np.random.choice(len(X), total, replace=False) if pick_randomly else list(range(0, total))\n",
        "\n",
        "    X = np.take(X, indexes, axis=0)\n",
        "    Y_ohe = np.take(Y_ohe, indexes, axis=0)\n",
        "    Y_pred = np.take(Y_pred, indexes, axis=0)\n",
        "    y_true = np.take(y_true, indexes, axis=0)\n",
        "    yp = np.take(yp, indexes, axis=0)\n",
        "\n",
        "    fig_height = rows * image_size_multiplier * 2\n",
        "    fig_width = columns * image_size_multiplier\n",
        "\n",
        "    fig = plt.figure(figsize=(fig_width, fig_height))\n",
        "    plt.subplots_adjust(bottom=0.1, top=1.0)\n",
        "    idx1 = 0\n",
        "    idx2 = 0\n",
        "    jdx = 0\n",
        "    for row in range(rows):\n",
        "        for column in range(columns):\n",
        "            if idx1 >= len(X):\n",
        "                break\n",
        "            img = X[idx1]\n",
        "            assert (len(img.shape) == 3 and img.shape[2] in [1, 3, 4]) or len(img.shape) == 2\n",
        "            ax = fig.add_subplot(rows * 2, columns, jdx + 1, xticks=[], yticks=[])\n",
        "            cmap = None\n",
        "            if (len(img.shape) == 3 and img.shape[2] == 1) or len(img.shape) == 2:\n",
        "                cmap = \"binary\"\n",
        "            if len(img.shape) == 3 and img.shape[2] == 1:\n",
        "                img = img.reshape((img.shape[0], img.shape[1]))\n",
        "            ax.imshow(img, cmap=cmap)\n",
        "            ax.set_xlabel(\"Predicted = %s, Actual = %s\" % (classes[yp[idx1]], classes[y_true[idx1]]))\n",
        "            idx1 += 1\n",
        "            jdx += 1\n",
        "\n",
        "        for column in range(columns):\n",
        "            if idx2 >= len(Y_pred):\n",
        "                break\n",
        "            yps = Y_pred[idx2]\n",
        "            ax = fig.add_subplot(rows * 2, columns, jdx + 1, xticks=[], yticks=[])\n",
        "            ind = np.arange(len(classes))\n",
        "            rects = ax.bar(ind, yps, 0.25, label='Labels')\n",
        "            ax.set_ylabel('Probability')\n",
        "            ax.set_yticks(np.arange(0, 1.2, 0.2))\n",
        "            ax.set_xticks(ind)\n",
        "            ax.set_xticklabels(classes, rotation=90, ha='left')\n",
        "            ax.legend()\n",
        "            ax.tick_params(axis='both', which='major', labelsize=8)\n",
        "            ax.tick_params(axis='both', which='minor', labelsize=6)\n",
        "            idx2 += 1\n",
        "            jdx += 1\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3qU1HlZi3-U",
        "colab_type": "code",
        "outputId": "53186b97-d94e-40da-c40a-1ad18b0b88ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1309
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "predict = model.predict_classes(X_test)\n",
        "\n",
        "d = {'pred': predict, 'true': np.argmax(Y_test,axis=1)} \n",
        "df = pd.DataFrame(data=d)\n",
        "df2 = df[(df.pred != df.true)]\n",
        "\n",
        "print(\"confusion matrix for 10 classes is below\")\n",
        "cm = confusion_matrix(y_test1,predict)\n",
        "print(cm)\n",
        "\n",
        "misclassified_image_count = df2.shape[0]\n",
        "print(\"Out of 10000 images \" + str(misclassified_image_count) +\" images are misclassified\")\n",
        "#print(df2)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion matrix for 10 classes is below\n",
            "[[ 976    0    1    0    0    1    1    1    0    0]\n",
            " [   0 1131    0    0    0    0    1    3    0    0]\n",
            " [   1    0 1028    0    0    0    1    2    0    0]\n",
            " [   0    0    2  997    0   10    0    0    0    1]\n",
            " [   0    0    0    0  975    0    1    0    1    5]\n",
            " [   0    0    0    1    0  890    1    0    0    0]\n",
            " [   3    1    0    0    0    1  953    0    0    0]\n",
            " [   0    3    8    0    0    1    0 1015    0    1]\n",
            " [   1    1    1    1    0    2    1    0  966    1]\n",
            " [   2    0    0    0    4    4    1    4    0  994]]\n",
            "Out of 10000 images 75 images are misclassified\n",
            "      pred  true\n",
            "115      9     4\n",
            "321      7     2\n",
            "445      0     6\n",
            "449      5     3\n",
            "450      5     3\n",
            "582      2     8\n",
            "684      2     7\n",
            "716      7     1\n",
            "938      5     3\n",
            "947      9     8\n",
            "965      0     6\n",
            "1014     5     6\n",
            "1112     6     4\n",
            "1156     5     7\n",
            "1226     2     7\n",
            "1232     4     9\n",
            "1247     5     9\n",
            "1260     1     7\n",
            "1415     6     8\n",
            "1878     3     8\n",
            "1901     4     9\n",
            "2130     9     4\n",
            "2293     6     9\n",
            "2308     5     3\n",
            "2326     5     0\n",
            "2329     2     0\n",
            "2380     0     9\n",
            "2414     4     9\n",
            "2447     9     4\n",
            "2462     0     2\n",
            "...    ...   ...\n",
            "4201     7     1\n",
            "4284     5     9\n",
            "4443     2     3\n",
            "4690     2     7\n",
            "4740     5     3\n",
            "4761     4     9\n",
            "4874     0     9\n",
            "5165     6     0\n",
            "5654     2     7\n",
            "5887     2     7\n",
            "5955     9     3\n",
            "6570     5     3\n",
            "6571     7     9\n",
            "6572     7     1\n",
            "6576     1     7\n",
            "6597     7     0\n",
            "6625     1     8\n",
            "6632     5     9\n",
            "7248     5     3\n",
            "8061     9     4\n",
            "8128     6     1\n",
            "8246     5     3\n",
            "8316     2     7\n",
            "8408     5     8\n",
            "8527     9     4\n",
            "9015     2     7\n",
            "9620     7     9\n",
            "9638     7     9\n",
            "9642     7     9\n",
            "9729     6     5\n",
            "\n",
            "[75 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}