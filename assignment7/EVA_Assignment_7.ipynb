{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_Assignment 7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8edSiHPi2N5l",
        "colab_type": "code",
        "outputId": "306e39dc-bd72-4941-f3c4-b339e1fec1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "!pip install graphviz\n",
        "import keras\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "#from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D\n",
        "from PIL import Image\n",
        "\n",
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "l = 10\n",
        "num_filter = 20"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hkvQwOH2cCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "39786839-2bdb-421f-8860-41a3a69ddc6d"
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztoDypc63gEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def space_to_depth_x2(x):\n",
        "    return tf.space_to_depth(x, block_size=2)\n",
        "  \n",
        "def space_to_depth_x4(x):\n",
        "    return tf.space_to_depth(x, block_size=4)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNnGm8Tv2fR1",
        "colab_type": "code",
        "outputId": "41ff130c-89a8-4b0b-999f-6c2906b553a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import layers\n",
        "input_layer = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "# Layer 1, 5x5 Separable , Depthwise\n",
        "Layer1 = SeparableConv2D(32, (5,5), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_layer)\n",
        "Layer1_normalized = BatchNormalization(name='norm_1')(Layer1)\n",
        "Layer1_normalized_activated = Activation('relu')(Layer1_normalized)\n",
        "\n",
        "Layer1_normalized_activated = Dropout(0.2)(Layer1_normalized_activated)\n",
        "\n",
        "# Layer 2,Normal Conv2d 5x5\n",
        "Layer2 = Conv2D(64, (5,5), strides=(1,1), padding='same', name='conv_2', use_bias=False)(Layer1_normalized_activated)\n",
        "Layer2_normalized = BatchNormalization(name='norm_2')(Layer2)\n",
        "Layer2_normalized_activated = Activation('relu')(Layer2_normalized)\n",
        "\n",
        "Layer2_normalized_activated = Dropout(0.2)(Layer2_normalized_activated)\n",
        "\n",
        "# Layer 3, Normal Conv2d 5x5\n",
        "Layer3 = Conv2D(64, (5,5), strides=(1,1), padding='same', name='conv_3', use_bias=False)(Layer2_normalized_activated)\n",
        "Layer3_normalized = BatchNormalization(name='norm_3')(Layer3)\n",
        "Layer3_normalized_activated = Activation('relu')(Layer3_normalized)\n",
        "Layer3_normalized_activated = Dropout(0.2)(Layer3_normalized_activated)\n",
        "\n",
        "Layers_1_3_concatenated = concatenate([Layer1_normalized_activated,Layer3_normalized_activated])\n",
        "\n",
        "# Layer 4, 5x5 Separable\n",
        "Layer4 = SeparableConv2D(64, (5,5), strides=(1,1), padding='same', name='conv_4', use_bias=False)(Layers_1_3_concatenated)\n",
        "Layer4_normalized = BatchNormalization(name='norm_4')(Layer4)\n",
        "Layer4_normalized_activated = Activation('relu')(Layer4_normalized)\n",
        "Layer4_normalized_activated = Dropout(0.2)(Layer4_normalized_activated)\n",
        "\n",
        "Layers_1_4_concatenated = concatenate([Layer1_normalized_activated,Layer4_normalized_activated])\n",
        "\n",
        "#Maxpool layer\n",
        "Layer_maxpool1 = MaxPooling2D(pool_size=(2, 2))(Layers_1_4_concatenated) #16\n",
        "\n",
        "#Bottleneck 1x1 to reduce params\n",
        "Layer_bottleneck1 = Conv2D(32, (1, 1), activation='relu')(Layer_maxpool1)\n",
        "\n",
        "# Layer 5, 3x3 Separable , Depthwise\n",
        "Layer5 = SeparableConv2D(64, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(Layer_bottleneck1)\n",
        "Layer5_normalized = BatchNormalization(name='norm_5')(Layer5)\n",
        "Layer5_normalized_activated = Activation('relu')(Layer5_normalized)\n",
        "Layer5_normalized_activated = Dropout(0.2)(Layer5_normalized_activated)\n",
        "\n",
        "Layers_1_4_5_concatenated = concatenate([Lambda(space_to_depth_x2)(Layer1_normalized_activated),Lambda(space_to_depth_x2)(Layer4_normalized_activated),Layer5_normalized_activated])\n",
        "\n",
        "# Layer 6, Normal Conv2d 5x5\n",
        "Layer6 = Conv2D(64, (5,5), strides=(1,1), padding='same', name='conv_6', use_bias=False)(Layers_1_4_5_concatenated)\n",
        "Layer6_normalized = BatchNormalization(name='norm_6')(Layer6)\n",
        "Layer6_normalized_activated = Activation('relu')(Layer6_normalized)\n",
        "Layer6_normalized_activated = Dropout(0.2)(Layer6_normalized_activated)\n",
        "\n",
        "Layers_3_4_5_6_concatenated = concatenate([Lambda(space_to_depth_x2)(Layer3_normalized_activated),Lambda(space_to_depth_x2)(Layer4_normalized_activated),Layer5_normalized_activated,Layer6_normalized_activated])\n",
        "\n",
        "# Layer 7, 3x3 Separable , Depthwise\n",
        "Layer7 = SeparableConv2D(64, (3,3), strides=(1,1), padding='same', name='conv_7', use_bias=False)(Layers_3_4_5_6_concatenated)\n",
        "Layer7_normalized = BatchNormalization(name='norm_7')(Layer7)\n",
        "Layer7_normalized_activated = Activation('relu')(Layer7_normalized)\n",
        "Layer7_normalized_activated = Dropout(0.2)(Layer7_normalized_activated)\n",
        "\n",
        "Layers_1_3_4_5_6_7_concatenated = concatenate([Lambda(space_to_depth_x2)(Layer1_normalized_activated),Lambda(space_to_depth_x2)(Layer3_normalized_activated),Lambda(space_to_depth_x2)(Layer4_normalized_activated),Layer5_normalized_activated,Layer6_normalized_activated,Layer7_normalized_activated])\n",
        "\n",
        "# Layer 8, 5x5 Separable Depthwise.\n",
        "Layer8 = SeparableConv2D(64, (5,5), strides=(1,1), padding='same', name='conv_8', use_bias=False)(Layers_1_3_4_5_6_7_concatenated)\n",
        "Layer8_normalized = BatchNormalization(name='norm_8')(Layer8)\n",
        "Layer8_normalized_activated = Activation('relu')(Layer8_normalized)\n",
        "Layer8_normalized_activated = Dropout(0.2)(Layer8_normalized_activated)\n",
        "\n",
        "Layers_1_4_7_8_concatenated = concatenate([Lambda(space_to_depth_x2)(Layer1_normalized_activated),Lambda(space_to_depth_x2)(Layer4_normalized_activated),Layer7_normalized_activated,Layer8_normalized_activated])\n",
        "\n",
        "Layer_maxpool2 = MaxPooling2D(pool_size=(2, 2))(Layers_1_4_7_8_concatenated) #8\n",
        "\n",
        "Layers_6_maxpool2_concatenated = concatenate([Lambda(space_to_depth_x2)(Layer6_normalized_activated),Layer_maxpool2])\n",
        "\n",
        "#Bottleneck 1x1 to reduce params\n",
        "Layer_bottleneck2 = Conv2D(64, (1, 1), activation='relu')(Layers_6_maxpool2_concatenated)\n",
        "\n",
        "# Layer 9\n",
        "Layer9 = Conv2D(64, (5,5), strides=(1,1), padding='same', name='conv_9', use_bias=False)(Layer_bottleneck2)\n",
        "Layer9_normalized = BatchNormalization(name='norm_9')(Layer9)\n",
        "Layer9_normalized_activated = Activation('relu')(Layer9_normalized)\n",
        "Layer9_normalized_activated = Dropout(0.2)(Layer9_normalized_activated)\n",
        "\n",
        "Layers_1_2_7_9_concatenated = concatenate([Lambda(space_to_depth_x4)(Layer1_normalized_activated),Lambda(space_to_depth_x4)(Layer2_normalized_activated),Lambda(space_to_depth_x2)(Layer7_normalized_activated),Layer9_normalized_activated])\n",
        "\n",
        "# Layer 10,5x5 Separable , Depthwise\n",
        "Layer10 = SeparableConv2D(64, (5,5), strides=(1,1), padding='same', name='conv_10', use_bias=False)(Layers_1_2_7_9_concatenated)\n",
        "Layer10_normalized = BatchNormalization(name='norm_10')(Layer10)\n",
        "Layer10_normalized_activated = Activation('relu')(Layer10_normalized)\n",
        "Layer10_normalized_activated = Dropout(0.2)(Layer10_normalized_activated)\n",
        "\n",
        "Layers_2_3_5_9_10_concatenated = concatenate([Lambda(space_to_depth_x4)(Layer2_normalized_activated),Lambda(space_to_depth_x4)(Layer3_normalized_activated),Lambda(space_to_depth_x2)(Layer5_normalized_activated),Layer9_normalized_activated,Layer10_normalized_activated])\n",
        "\n",
        "# Layer 11, Normal Conv2d 3x3\n",
        "Layer11 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(Layers_2_3_5_9_10_concatenated)\n",
        "Layer11_normalized = BatchNormalization(name='norm_11')(Layer11)\n",
        "Layer11_normalized_activated = Activation('relu')(Layer11_normalized)\n",
        "Layer11_normalized_activated = Dropout(0.2)(Layer11_normalized_activated)\n",
        "\n",
        "Layers_3_4_5_7_10_11_concatenated = concatenate([Lambda(space_to_depth_x4)(Layer3_normalized_activated),Lambda(space_to_depth_x4)(Layer4_normalized_activated),Lambda(space_to_depth_x2)(Layer5_normalized_activated),Lambda(space_to_depth_x2)(Layer7_normalized_activated),Layer10_normalized_activated,Layer11_normalized_activated])\n",
        "\n",
        "# Layer 12 -  5x5 Separable , Depthwise\n",
        "Layer12 = SeparableConv2D(64, (3,3), strides=(1,1), padding='same', name='conv_12', use_bias=False)(Layers_3_4_5_7_10_11_concatenated)\n",
        "Layer12_normalized = BatchNormalization(name='norm_12')(Layer12)\n",
        "Layer12_normalized_activated = Activation('relu')(Layer12_normalized)\n",
        "Layer12_normalized_activated = Dropout(0.2)(Layer12_normalized_activated)\n",
        "\n",
        "Layers_4_7_10_12_concatenated = concatenate([Lambda(space_to_depth_x4)(Layer4_normalized_activated), Lambda(space_to_depth_x2)(Layer7_normalized_activated), Layer10_normalized_activated, Layer12_normalized_activated])\n",
        "\n",
        "Layer_bottleneck3 = Conv2D(10, (1,1), strides=(1,1), padding='same', name='conv_f1', use_bias=False)(Layers_4_7_10_12_concatenated)\n",
        "#Layer_f2 = Conv2D(10, (8,8), strides=(1,1), name='conv_f2', use_bias=False)(Layer_bottleneck3)\n",
        "\n",
        "img_output = AveragePooling2D(8,8)(Layer_bottleneck3)\n",
        "img_output=  Flatten()(img_output)\n",
        "img_output = Activation('softmax')(img_output)\n",
        "\n",
        "model = Model(inputs=[input_layer], outputs=[img_output])\n",
        "model.summary()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_1 (SeparableConv2D)        (None, 32, 32, 32)   171         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1 (BatchNormalization)     (None, 32, 32, 32)   128         conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 32, 32, 32)   0           norm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 32)   0           activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_2 (Conv2D)                 (None, 32, 32, 64)   51200       dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "norm_2 (BatchNormalization)     (None, 32, 32, 64)   256         conv_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 32, 32, 64)   0           norm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 32, 32, 64)   0           activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_3 (Conv2D)                 (None, 32, 32, 64)   102400      dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "norm_3 (BatchNormalization)     (None, 32, 32, 64)   256         conv_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 32, 32, 64)   0           norm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 32, 32, 64)   0           activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_56 (Concatenate)    (None, 32, 32, 96)   0           dropout_13[0][0]                 \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_4 (SeparableConv2D)        (None, 32, 32, 64)   8544        concatenate_56[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_4 (BatchNormalization)     (None, 32, 32, 64)   256         conv_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 32, 32, 64)   0           norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 32, 32, 64)   0           activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_57 (Concatenate)    (None, 32, 32, 96)   0           dropout_13[0][0]                 \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 16, 16, 96)   0           concatenate_57[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 32)   3104        max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv_5 (SeparableConv2D)        (None, 16, 16, 64)   2336        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "norm_5 (BatchNormalization)     (None, 16, 16, 64)   256         conv_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 16, 16, 64)   0           norm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 64)   0           activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_67 (Lambda)              (None, 16, 16, 128)  0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_68 (Lambda)              (None, 16, 16, 256)  0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_58 (Concatenate)    (None, 16, 16, 448)  0           lambda_67[0][0]                  \n",
            "                                                                 lambda_68[0][0]                  \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_6 (Conv2D)                 (None, 16, 16, 64)   716800      concatenate_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_6 (BatchNormalization)     (None, 16, 16, 64)   256         conv_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 16, 16, 64)   0           norm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_69 (Lambda)              (None, 16, 16, 256)  0           dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_70 (Lambda)              (None, 16, 16, 256)  0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 64)   0           activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_59 (Concatenate)    (None, 16, 16, 640)  0           lambda_69[0][0]                  \n",
            "                                                                 lambda_70[0][0]                  \n",
            "                                                                 dropout_17[0][0]                 \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_7 (SeparableConv2D)        (None, 16, 16, 64)   46720       concatenate_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_7 (BatchNormalization)     (None, 16, 16, 64)   256         conv_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 16, 16, 64)   0           norm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 16, 16, 64)   0           activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_71 (Lambda)              (None, 16, 16, 128)  0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_72 (Lambda)              (None, 16, 16, 256)  0           dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_73 (Lambda)              (None, 16, 16, 256)  0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_60 (Concatenate)    (None, 16, 16, 832)  0           lambda_71[0][0]                  \n",
            "                                                                 lambda_72[0][0]                  \n",
            "                                                                 lambda_73[0][0]                  \n",
            "                                                                 dropout_17[0][0]                 \n",
            "                                                                 dropout_18[0][0]                 \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_8 (SeparableConv2D)        (None, 16, 16, 64)   74048       concatenate_60[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_8 (BatchNormalization)     (None, 16, 16, 64)   256         conv_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 16, 16, 64)   0           norm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_74 (Lambda)              (None, 16, 16, 128)  0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_75 (Lambda)              (None, 16, 16, 256)  0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 64)   0           activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_61 (Concatenate)    (None, 16, 16, 512)  0           lambda_74[0][0]                  \n",
            "                                                                 lambda_75[0][0]                  \n",
            "                                                                 dropout_19[0][0]                 \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_76 (Lambda)              (None, 8, 8, 256)    0           dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 8, 8, 512)    0           concatenate_61[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_62 (Concatenate)    (None, 8, 8, 768)    0           lambda_76[0][0]                  \n",
            "                                                                 max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 8, 8, 64)     49216       concatenate_62[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_9 (Conv2D)                 (None, 8, 8, 64)     102400      conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "norm_9 (BatchNormalization)     (None, 8, 8, 64)     256         conv_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 8, 8, 64)     0           norm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_77 (Lambda)              (None, 8, 8, 512)    0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_78 (Lambda)              (None, 8, 8, 1024)   0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_79 (Lambda)              (None, 8, 8, 256)    0           dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 8, 8, 64)     0           activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_63 (Concatenate)    (None, 8, 8, 1856)   0           lambda_77[0][0]                  \n",
            "                                                                 lambda_78[0][0]                  \n",
            "                                                                 lambda_79[0][0]                  \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_10 (SeparableConv2D)       (None, 8, 8, 64)     165184      concatenate_63[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_10 (BatchNormalization)    (None, 8, 8, 64)     256         conv_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 8, 8, 64)     0           norm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 8, 8, 64)     0           activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_80 (Lambda)              (None, 8, 8, 1024)   0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_81 (Lambda)              (None, 8, 8, 1024)   0           dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_82 (Lambda)              (None, 8, 8, 256)    0           dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_64 (Concatenate)    (None, 8, 8, 2432)   0           lambda_80[0][0]                  \n",
            "                                                                 lambda_81[0][0]                  \n",
            "                                                                 lambda_82[0][0]                  \n",
            "                                                                 dropout_21[0][0]                 \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_11 (Conv2D)                (None, 8, 8, 64)     1400832     concatenate_64[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_11 (BatchNormalization)    (None, 8, 8, 64)     256         conv_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 8, 8, 64)     0           norm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_83 (Lambda)              (None, 8, 8, 1024)   0           dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_84 (Lambda)              (None, 8, 8, 1024)   0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_85 (Lambda)              (None, 8, 8, 256)    0           dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_86 (Lambda)              (None, 8, 8, 256)    0           dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 8, 8, 64)     0           activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_65 (Concatenate)    (None, 8, 8, 2688)   0           lambda_83[0][0]                  \n",
            "                                                                 lambda_84[0][0]                  \n",
            "                                                                 lambda_85[0][0]                  \n",
            "                                                                 lambda_86[0][0]                  \n",
            "                                                                 dropout_22[0][0]                 \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_12 (SeparableConv2D)       (None, 8, 8, 64)     196224      concatenate_65[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_12 (BatchNormalization)    (None, 8, 8, 64)     256         conv_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 8, 8, 64)     0           norm_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_87 (Lambda)              (None, 8, 8, 1024)   0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_88 (Lambda)              (None, 8, 8, 256)    0           dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 8, 8, 64)     0           activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_66 (Concatenate)    (None, 8, 8, 1408)   0           lambda_87[0][0]                  \n",
            "                                                                 lambda_88[0][0]                  \n",
            "                                                                 dropout_22[0][0]                 \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_f1 (Conv2D)                (None, 8, 8, 10)     14080       concatenate_66[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 1, 1, 10)     0           conv_f1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 10)           0           average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 10)           0           flatten_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,936,203\n",
            "Trainable params: 2,934,731\n",
            "Non-trainable params: 1,472\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj3OjhqsadTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "210034d1-9f39-4b1d-fe1e-28a2e32bc396"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from IPython.core.display import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "Image( filename ='model_plot.png')\n",
        "\n",
        "img = mpimg.imread('model_plot.png')\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJMAAAD8CAYAAABzR5aaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGIxJREFUeJztnXtwVdW9xz8/QoDwKETxhgwoLwlW\nWyvIKFTKyEUeeumVmQaH1lHqo6BwRdpp78UrtlVhxtqORUawBOqtdVQErAN6FYUW55bWB8FGUAwk\nRAQkvAyvmETQ/u4fe5/DSUhy9jlnP89Zn5k9WXuddfb+Hfy6116v7xJVxWBwgw5BB2DIHoyYDK5h\nxGRwDSMmg2sYMRlcw4jJ4Bq+i0lEJonIThGpFpF5ft/f4B3iZz+TiOQBu4DxwH5gC/B9Vd3hWxAG\nz/D7yXQVUK2qNap6GlgJ3OhzDAaP6Ojz/foC+xLO9wNXJxYQkRnADIBu3bpdeckll7h28+PHj9PY\n2EhxcbFr18wFtm7delRVL0hWzm8xJUVVy4AygBEjRmh5ebln9yopKWHXrl2eXT9bEJFPnJTzu5r7\nFLgw4byfnRcIRkju4reYtgBDRGSgiHQCpgHrfI6hVaZOnRp0CJHH12pOVb8Ukf8AXgfygKdU9UM/\nYzB4h+/vTKr6KvCq3/dNxurVq4MOIfKYHnCDa4SuNecmF198MStXrmTEiBHxvPbejczTKTOyWkzV\n1dXn5BnBeEdOVXMiEnQIWU1OicnMd/eWnBKTwVuMmAyukVNiampqanZeUlICmHcpt8gpMeXl5XHq\n1Kn4eVVVVYDRZB85Jab8/Hx69OgRP4+9kKf7Yj548GBX4soWckpMbrN79+6gQwgVOSWm2KQ400Xg\nDTklptraWsC8cHtFqIdTampqfJ1nZIZaMiPUYurbty9btmzh2LFjHD16lCFDhrBnzx7H3+/atSsN\nDQ00NTVx8OBBBgwY4FmshpCLqXPnzmzfvj1+noqQABoaGgDo0qWLEZIP5NQ707e//e2gQ8hqslJM\nzzzzDHPnzo2fd+hg/cy///3vQYV0DosWLQLg6NGjAFRUVLRbftWqVZ7HlCm+ruhNlXSXOqXz0u73\ny3eqMe7fv5+33nrLo2jaR0S2quqIpOWyUUwGd3EqpqTVnIg8JSKHReSDhLzzRGSDiFTZfwvtfBGR\nxbYpxTYRGZ7wnel2+SoRmZ7uD8uEpqYmFi5cGD+P9Tf52e+UeK/KykpEhN/85je+3d9TVLXdAxgD\nDAc+SMh7FJhnp+cBv7LTNwCvAQKMBN6x888Dauy/hXa6MNm9r7zySvWC48ePe3LdbAUo1yT/rVQ1\n+ZNJVf8PqGuRfSPwtJ1+GpiSkP9HO4a3gV4iUgxMBDaoap2qHgM2AJNSEb0T5s1z5tDTs2dPt29t\nIP3WXJGq1trpg0CRnW7NmKJvO/nnICIzRKRcRMqPHDmSUlCPPPJIu5+vXbu2WTUXo1+/fu1+b/bs\n2UDzMb3JkyenFFsukHHXgP0YdO0tXlXLVHWEqo644IKkxhspceONN3L//ffHz2OT4z79tG27g/Ly\ncpYsWQI0f9955ZVXXI0tG0hXTIfs6gv772E7vy1jilAZVsSIGVckPnFakrjmLsa2bds8iynKpCum\ndUCsRTYdWJuQf6vdqhsJnLCrw9eBCSJSaLf8Jth5rtByOq7XXH755Tz55JO+3jMSJHtDB54HaoEz\nWO86dwDnA38GqoCNwHl2WQGWALuB7cCIhOvcDlTbx21OWgdeteZaYv0zGNoCh625nOm03Lt3Lxdd\ndJEr18o1XOu0zBaMkLwn1FNQ2mPq1KmtjqdlakyxdetW+vbtS58+fYzJRYpkbTW3c+dOhg4d6nJE\nuUnOV3NuCMnMFU+NSIrp888/T/u7c+bMcVw2zE/tMBJJMXXr1i3t7y5evNiVGJyOA6bCpEmuD1f6\nSiTFFAaSjQMCjB8/PqVrrl+/Pt1wQoERk4ds2LAh5e98/PHH8XRs7HDKlCnxvB//+MeZB+YRRkxp\n0L1795TK//Wvf01a5siRI5w5c4aBAwfG8+rr6zl58iQbN27k2LFjAPz2t79NLVgfiWw/U5DU19en\nVP473/lO0jKtzZA4cOBAWvcLipx7MgXR3P/1r3/t+z2DIOfElKy5P2fOHObOnUvHjh2TTppzys9+\n9jPHZSsrK+Ot1bVr18bz58+f70osXpK1PeAG93DaA57T70xO1669++67XHXVVYA7Y3LpmnGEfjzQ\nyTyVoI5k85lOnjyZ4syccxk3bpyqqg4ePDie9prly5erqmpDQ0Oz/Pr6+nj6hRde8CUWJ2DmMxnc\nIucHeg3+k/Ni6tKlS3ziXJcuXXxZUxczoSgsLIznvf66a1PiAyPnq7kgTC5GjRqVVrdDUC/gxrjC\nZf7whz/wwx/+MOgwAsFN44oLRWSTiOwQkQ9F5F47P5LmFeniVEjbt2+nd+/ejBo1Kq37fPHFF83O\nm5qa2Lt3bzT2EE7W3AOKgeF2ugewC7gUH8wr/FrqFAVKS0v11KlTgdwbF40ralX1PTt9CvgIyycg\nlOYV2crq1atTnq3gNym15kRkADAMeAePzCsyMa4II1/72tcAa8wvlm6NO++805P7J+4V4zWOxSQi\n3YEXgbmqejLxM/tR6MqbvHpoXBEEJ09a/1SLFy+OpwHOnDnDN77xjfgshhUrVnhy/8S9YrzGkZhE\nJB9LSM+q6p/s7KwwrwiK/Px8Pvjgg0AWLSTaYbuJk9acAL8HPlLVxxI+CpV5hcE53/zmNz25rpNZ\nA9cAtwDbRSTmL/zfwCPAKhG5A/gEuMn+7FWsFl010ADcBqCqdSLyMLDFLveQqrZ0pIs0IhKJ5VFe\nxWk6LQ1JMQO9Bt/J6clxXjN58mQqKysZNmxYm2WCGm/zwpTDVHOGpJhqzuA7RkwG1zBiiiibNm0K\nOoRzMGKKKGPHjg06hHMwYgqQK664wnHZzZs3x9NVVVVehJMxWS8mJ+v8/SZxwDcZX375JQCjR4+O\n58XcUR5++GF3A8sUJ5OegjqycXLcrFmzUv5Ofn6+lpeX67Jly3Tfvn0eRNU+OJwcF7hg2juyQUyL\nFi0KOoSMcSqmrK/mgubee+91VO7o0aMsXLiQxsZGCgoKHH1n9uzZXHzxxZmE5yqmBzxDXn75Zb77\n3e8GHYanGOMKn2gppPbGvPbv3++aTU9r42ejR49u1urzHSd1YVBHFN+ZGhsb9cSJEzp27Nh4OkZ7\nZhR1dXXx9MyZM/X06dNp3X/p0qVpfa89MMYVBrcwA70B8KMf/ajNz+6++24fIwkGI6YM+eqrrygo\nKGD9+vU8++yzXH755a2Wy4XNDk01lwFeL9levXo1Xbt2paGhwdP7JMO05nzA6YzEmpoaBg0alNY9\nghZSKjhZ6tRFRN4Vkfdt44oH7fyBIvKObVDxgoh0svM72+fV9ucDEq51n52/U0QmevWjwsS2bdsY\nNGgQU6ZM4b333nPtunfddVc8XVtb6+q10yZZcw/LgKK7nc7HWho+ElgFTLPzfwfcbadnAb+z09OA\nF+z0pcD7QGdgINY+vnnt3TuKXQPJqK2tDTqElMFF4wpV1ZhFfr59KPCvwBo7v6VxRczQYg0wzl7I\neSOwUlW/UNWPsdbVXZWC7rOCPn36BB2CZzhdHp5nL8A8jOVeshs4rqpf2kUSTSjiBhX25yewdhvP\nOeMKEaFz585AZgbxsWukMv8pCByJSVW/UtUrsPwBrgIu8SogzSLjik6dOrFw4UIAJk5M/RXxBz/4\nAXDWAKyioqK94oGTUmtOVY+LyCZgFJbvUkf76ZNoQhEzqNgvIh2BnsBn5KBxRaILXJcuXeLpBQsW\nOPr+c88953pMXuKkNXeBiPSy0wXAeCzDr01AqV2spXFFzNCiFPiL/RK3Dphmt/YGAkOAd936IYbg\ncfJkKgaeFpE8LPGtUtVXRGQHsFJEFgD/wHJKwf77jIhUA3VYLTpU9UMRWQXsAL4EZqvqV+7+nGCo\nrKzkkks8q/kjg+kBT4GSkhJ27doVdBi+YwZ6PSAdId13330eRBJOzHBKmhQVFXH48GFUNT5Gt2bN\nGkpLS5uVC/1OTC5ixJQmhw4diqdzSTDtYao5B+zduzfja7S2LX0QW7x6iRGTA2Ib9bSFE1G03Jb+\n5ptvprXGT2uLRu+///6kZcKAac25QFS8LNPFtOZ8YM+ePUDyTaRzBSOmNNm8eTMDBgxwXN7JrMyp\nU6eybNmy+Pm3vvWtdEILDCOmNJg2bRpf//rXXb3mZZddBsDMmTPjeX5spOgm5p0pJGzevLmZ00mY\nMO9MEaOlkPzc88QtjJhCRmyqSmJHaGxyXNjJOTHl5eVl9P3bb7/dpUhap6mpCYBJk85uxddyd8yw\nknPDKV991f6sl+LiYmpra9v8/KmnnnItliA2m/aSnBNTMtoTUmt06NCBf/7zn2ndq7CwkOXLl1NX\nV9dsW/qoknPVnNukKySAsrIyVDUrhARGTAYXMWJyiUyWZU2dOpXKykpEhPXr10f2SWU6LUPAhRde\nyMCBAykqKkpe2GbBggUMHTrUw6jOYowrQkpBQQGNjY3N8vbt29dG6WiRyu7heSLyDxF5xT7PGeOK\npqam+DbysblLY8aMaTaP6Z577gHOTqRry0yisbExXkZEGDNmTKt9X7Eyd9xxh6MYQzGO58SQwK4K\nfwI8B7xinxvjCgds3Lgxnk70rYwSuOkDLiL9gH8DVtjngjGucMS4cePiab9frGNPTr8ceJ1Wc4uA\n/wRinSrn45FxRbaxdOlSIJjxNbUbV63NRnj++ecBa4JfsmnJTnGyPHwycFhVt7pyx+T3yxoXFIBZ\ns2YB4Rtfiy1wGDBgAG+88YYr13TyZLoG+HcR2QOsxKreHsc2rrDLtGZcQTrGFZpFLihhpnfv3vG0\nW0vbnZh93aeq/VR1ANYL9V9U9WaMcYVvvPXWW21+tmrVqoyuXVlZmdH3E8mkB/y/gJ/YBhXn09y4\n4nw7/yfAPLCMK7BagDuA9WSRcUUyYkvEe/Xqldb3R40a5ahcogmr0zV5rT2V0l3PZ3rAQ0a/fv3Y\nv3+/r/c8ffo0nTp1avNzM23XR9xcmeu3kIB2hZQKZjglgWuvvZY333yzWTo2ge3tt99m5MiRzcrH\nJqqpanzHJhGhurqawYMHtzv5LcyT3NLFVHOGpJhqznAOGzdu9PT6Rkwu89BDDzV7hwrTypLrrrvO\n0+sbMbnMz3/+82beA370fJeVlXl+DycYMUWE9lqMM2bM8DGStjFiwh0zr9ZI3FEgJoZbbrml1UUI\n69ata2Za0ZIwN5TiOJmnEtQRpflM27Ztc7Svbn19vV599dWOr1tRUZFJWK6A2aPX4Bama8AFHnzw\nwaBDiBRGTO3wi1/8ArBWj2RKWFpcXmLE5AA3Vo+EpcXlJWZsLgVajrX97W9/45prrkn5Ok7H5b73\nve/x4osvpnz9wHDylh7UEcbW3JIlS/TAgQO6YMECVVU9cOBAwBF5D6Y1Z3AL05oz+I4RU5o89thj\nTJliLRXs1q1bwNGEA1PNpUmqrm9RngxnjCs8Jsri8Aqny8P3iMh2EakQkXI77zwR2SAiVfbfQjtf\nRGSxbVCxTUSGJ1xnul2+SkSmt3W/KBPbAgMs8/nPPvssuGD8xkmTD9gD9G6R9ygwz07PA35lp28A\nXgMEGAm8Y+efB9TYfwvtdGF79w1j10CuYEkjnnbPuKINEg0qWhpX/NGO422slb/FwERgg6rWqeox\nYAMwqeVFDeFA03iXdiomBd4Qka0iEhsXKFLVmDXtQSBme9aWQUXOGFf06dMHCNeU3WT88pe/zPga\nTl/AR6vqpyLyL8AGEWm2plhVVURcaRbaYp0ByTcNzIRMLJeTcfr0acCasltbW0txcXGbZfPy8pJ6\nk/uBG2Jy9GRS1U/tv4eBl7B8lQ7Z1Rf238N28bYMKkJlXOGVkADq6uri6faEBMlN7qOEE0udbiLS\nI5YGJgAf0NygoqVxxa12q24kcMKuDl8HJohIod3ym2DnGbIEJ0+mImCziLyP5Vryv6q6HngEGC8i\nVcB19jnAq1gttWpgOZYtIapaBzwMbLGPh+y8rCRSo/0ukTM94Nm+j66XmIHeFhgheY8ZTkmT+fPn\ns2DBAgAmTpxIRUUFY8aMabN8Lgy/GDGlSUxIffr04eDBgwFHEw5ypprzCiOks+SUmJL5P0apxzqM\n5JSYkhE2e+WokVNiuummm4IOIavJKTGly/z58wGYPn06K1asCDia8GJacw6IbQ//9NNPM3z4cEpK\nStrtBshVcqYH3JA+pgfcIy677LKgQwgtRkwOueuuuwD48MMP43nXX399UOGEElPNGZJiljqliIhQ\nWlqavGACuTDelhJOVh0Edfi5OgVrnrs+88wz8bzS0lLf7h9mcLg6xTyZbLSV6t48eVLDvIAbXMOI\nKYFHH30UsFaXXHvttXFjCoMzTGsugVwyo0gF05pLg3TF0b17d+rr612OJno4Na7oJSJrRKRSRD4S\nkVHGuOLsRoP19fUMHz682Rq5PXv2xLehf+CBBygqKjqnTCIPPvhgfBPBoqKiVsuEHidNPiwvgTvt\ndCegF8a4ImfALeMKEekJjMHe0FlVT6vqcSJuXNGjR4+gbp21OKnmBgJHgP8RkX+IyAp7ZW9kjSsO\nHjzIqVOnfL9vbCpL4g5NsblS2YATMXUEhgNPquow4HPs7eVj2I9C14wrRKRcRMqPHDnixiXPIeZS\n4jf9+vUD4NZbb40L6uWXXw4kFi9wIqb9wH5Vfcc+X4MlrkgbVwRBdXU1YE2yU7tL5v333w8yJFdJ\nKiZVPQjsE5GhdtY4YAfGuMLQAqf9TPcAz4pIJ6xW2G1YQlwlIncAnwCx2fqvYrXoqoEGuyyqWici\nMeMKyHLjCjfp378/n3zySdBhJCWnesBPnDhBz549XbueV6hqu9uo+o2ZttsKURAStL8fb5gxwykh\noaioiEOHDgFQWlrKmjVr2hwrDOuYYE5VcwA7duzg0ksvdfWa2UR+fj5nzpxplmequTaIkpD8ru76\n9et3jpBSIefEFCX8qjViZrGxgesTJ06kdR0jpnYYO3Zs0CH4QocOzWWQbkPFiKkdNm3aFHQI55Cu\nH5QfVaYRU0hJ/I8vIsyZMweAJ554gpKSEgBOnjx5TpmVK1fG8zp2PNtYT6wy161b54m4cq41FxVE\nhIKCAhoaGtL6fmNjIwUFBW7FYqbtRplM/yd3S0ipYKq5LKV///6+39M8mbKA4uJiamtraWxs5Oqr\nr6a4uJitW7f6HocRUxZQW2tNeC0oKGDbtm2BxWHEFAFSXc8Xw+8xPCOmCLB69ep4U76xsTE+l7yq\nqoohQ4YEGVozjJgiQmutuzAJCUxrzuAiRkwR4vXXm0+ZTzTWSPe9yk1MD3hESEcsbr2Amx7wNqip\nqWHQoEFBh5EybQnj+PHj9OrVy+doWsfJ8vChIlKRcJwUkblRNa5IFFJYp7+mQmFhIa+99hpwdnC4\noqIirWs9/vjjmQXjxJAgdgB5WEvB+2OMK3IG3DKuaME4YLeqfkLEjSsyYebMmUGHkDZezmtKVUzT\ngOftdGSNKzJl2bJlaX/3zjvvbGZMnwqzZ89uNb+hoYHnn7f+s5w+fRqApUuXUlZWFt9Db+1aa8G1\netngcvL40rO+TEexRARwvMXnx+y/rwCjE/L/DIwAfgrMT8h/APhpK/eZAZQD5RdddJFXT+7IAejN\nN98c1L1dt26+HnhPVQ/Z54dEpFhVa1Mwrri2Rf6brYi7DCgDq2sghfiyGg1xF06MVKq573O2igNj\nXGFogVNPy27AeOBPCdmPAONFpAq4zj4Hy7iiBsu4YjkwCyzjCiBmXLEFY1zhKhMmTADgtttuY926\ndYHEYHrADUkxK3oNvpNzwylRo0OHDhw4cOAc60QRQVV54oknWLRoEcOGDWvzGn719JtqzpAUU80Z\nfMeIKYKEddaDEVMEqampCTqEVjFiMriGEVMWkDgToLKyMrA4jJiygJdeeomuXbvGzz///PNA4gh1\n14CInAJ2Bh2HD/TGmpERVvqratLtIsLeabnTSf9G1BGR8mz4naaaM7iGEZPBNcIuprKgA/CJrPid\noX4BN0SLsD+ZDBHCiMngGqEVk4hMEpGd9srgecm/ES5E5EIR2SQiO0TkQxG5186P5EpoRzhZwuL3\ngbVyeDcwCGuJ1fvApUHHleJvKAaG2+kewC7gUnxYCR3UEdYn01VAtarWqOppYCXWSuHIoKq1qvqe\nnT4FfIS16DRrV0KHVUxZs/oXQEQGAMOAd8jildBhFVPWICLdgReBuap6MvEzteqxrOmbCauYHG1b\nH3ZEJB9LSM+qamzN4SG7+iKFldCR+LcIq5i2AENEZKC9Y/k0rJXCkUGsSUa/Bz5S1ccSPsreldBB\ntwDaaQ3dgNUC2g3cH3Q8acQ/GqsK2wZU2McNwPlYZh5VwEbgPLu8AEvs37sdGJFwrduxVkhXA7cF\n/dvaOsxwisE1wlrNGSKIEZPBNYyYDK5hxGRwDSMmg2sYMRlcw4jJ4Br/Dx4zcc/Piz9XAAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apCwOjvZ4Kts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "(model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLaFy2AO4TLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "850a8457-3ade-4b64-fe7e-ffef10b9b467"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=50,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 72s 1ms/step - loss: 1.3520 - acc: 0.5095 - val_loss: 1.2382 - val_acc: 0.5664\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9494 - acc: 0.6589 - val_loss: 1.2103 - val_acc: 0.5942\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7690 - acc: 0.7279 - val_loss: 0.8914 - val_acc: 0.6931\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.6588 - acc: 0.7687 - val_loss: 0.9719 - val_acc: 0.6765\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.5793 - acc: 0.7981 - val_loss: 1.2326 - val_acc: 0.6218\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.5132 - acc: 0.8205 - val_loss: 0.7389 - val_acc: 0.7536\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.4620 - acc: 0.8407 - val_loss: 1.1173 - val_acc: 0.6442\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.4211 - acc: 0.8533 - val_loss: 0.6485 - val_acc: 0.7890\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.3816 - acc: 0.8660 - val_loss: 0.6971 - val_acc: 0.7786\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.3434 - acc: 0.8799 - val_loss: 0.7813 - val_acc: 0.7629\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.3182 - acc: 0.8870 - val_loss: 0.6612 - val_acc: 0.7965\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.2836 - acc: 0.8992 - val_loss: 0.8454 - val_acc: 0.7528\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.2648 - acc: 0.9056 - val_loss: 0.7076 - val_acc: 0.7830\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.2332 - acc: 0.9182 - val_loss: 0.7422 - val_acc: 0.7862\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.2154 - acc: 0.9243 - val_loss: 0.7266 - val_acc: 0.7943\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1900 - acc: 0.9323 - val_loss: 0.7008 - val_acc: 0.8000\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1740 - acc: 0.9378 - val_loss: 0.6936 - val_acc: 0.8002\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1601 - acc: 0.9428 - val_loss: 0.7012 - val_acc: 0.7992\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1522 - acc: 0.9458 - val_loss: 0.8764 - val_acc: 0.7733\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1387 - acc: 0.9506 - val_loss: 0.8517 - val_acc: 0.7883\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1268 - acc: 0.9550 - val_loss: 0.7994 - val_acc: 0.7967\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1201 - acc: 0.9576 - val_loss: 0.8104 - val_acc: 0.7906\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1077 - acc: 0.9619 - val_loss: 1.0473 - val_acc: 0.7536\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1048 - acc: 0.9639 - val_loss: 0.7542 - val_acc: 0.8091\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0957 - acc: 0.9657 - val_loss: 0.6915 - val_acc: 0.8226\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0948 - acc: 0.9669 - val_loss: 0.9570 - val_acc: 0.7909\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0914 - acc: 0.9674 - val_loss: 0.7897 - val_acc: 0.8166\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0863 - acc: 0.9700 - val_loss: 0.7629 - val_acc: 0.8266\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0834 - acc: 0.9707 - val_loss: 0.8573 - val_acc: 0.8029\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0767 - acc: 0.9726 - val_loss: 0.6971 - val_acc: 0.8283\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0734 - acc: 0.9747 - val_loss: 0.7256 - val_acc: 0.8158\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0679 - acc: 0.9759 - val_loss: 0.9474 - val_acc: 0.7966\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0685 - acc: 0.9755 - val_loss: 0.8923 - val_acc: 0.8152\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0655 - acc: 0.9772 - val_loss: 1.0324 - val_acc: 0.7931\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0690 - acc: 0.9761 - val_loss: 1.2388 - val_acc: 0.7478\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0661 - acc: 0.9773 - val_loss: 0.9406 - val_acc: 0.8012\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0536 - acc: 0.9809 - val_loss: 0.8771 - val_acc: 0.8095\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0588 - acc: 0.9784 - val_loss: 1.0488 - val_acc: 0.7852\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0551 - acc: 0.9810 - val_loss: 0.8950 - val_acc: 0.8089\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 67s 1ms/step - loss: 0.0588 - acc: 0.9800 - val_loss: 0.8441 - val_acc: 0.8170\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.0552 - acc: 0.9810 - val_loss: 0.9467 - val_acc: 0.8057\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.0476 - acc: 0.9841 - val_loss: 0.8715 - val_acc: 0.8155\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.0518 - acc: 0.9819 - val_loss: 0.7900 - val_acc: 0.8280\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.0493 - acc: 0.9833 - val_loss: 0.7532 - val_acc: 0.8387\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.0469 - acc: 0.9833 - val_loss: 0.9460 - val_acc: 0.8150\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.0447 - acc: 0.9844 - val_loss: 0.9128 - val_acc: 0.8087\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.0489 - acc: 0.9828 - val_loss: 0.8895 - val_acc: 0.8215\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.0456 - acc: 0.9845 - val_loss: 0.8990 - val_acc: 0.8210\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.0433 - acc: 0.9854 - val_loss: 0.8077 - val_acc: 0.8320\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 68s 1ms/step - loss: 0.0467 - acc: 0.9839 - val_loss: 0.7611 - val_acc: 0.8346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb3375bf7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTfZIGrf4Uyd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ca27c1ba-301e-4a31-dd0e-1ede846b0c70"
      },
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"CIFR_densenet_Basic_model2.h5\")\n",
        "print(\"Saved the model to disk\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 5s 488us/step\n",
            "Test loss: 0.7610598748207092\n",
            "Test accuracy: 0.8346\n",
            "Saved the model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiyb9TlVGsZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"CIFR_densenet_Basic_model2.h5\")\n",
        "print(\"Saved the model to disk\")\n",
        "from google.colab import files\n",
        "\n",
        "files.download('CIFR_densenet_Basic_model2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9A3pesKbUJ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df85079a-9a3e-48d8-cb16-34f67c82403b"
      },
      "source": [
        "# Some more changes can be definitely done to improve accuracy here, will attempt once I get some more time\n",
        "print(\"Best validation accuracy is : 0.8387\")\n",
        "print(\"Best training accuracy is : 0.9839\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best validation accuracy is : 0.8387\n",
            "Best training accuracy is : 0.9839\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}